{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bpT1IPG9EDnn"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, RecordEpisodeStatistics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "import wandb\n",
    "import os\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "\n",
    "GLOBAL_SEED = 100 # Use 42 for consistency, or any arbitrary fixed number\n",
    "def set_seeds(seed_value):\n",
    "    \"\"\"Sets a fixed seed for reproducibility across all random components.\"\"\"\n",
    "    torch.manual_seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LG9ePLSSEPHr"
   },
   "outputs": [],
   "source": [
    "# Define a Transition named tuple for Experience Replay\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A fixed-size buffer to store experience tuples.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Retrieve a random batch of transitions.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lVOXCrfzFKUK"
   },
   "outputs": [],
   "source": [
    "# --- 1. A2C MODEL ARCHITECTURE ---\n",
    "# A single network that branches into Actor (policy) and Critic (value) heads.\n",
    "class A2CNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network model for the Advantage Actor-Critic (A2C) Agent.\n",
    "    It takes the state as input and outputs the action probabilities (Actor)\n",
    "    and the state value estimate (Critic).\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, config , hidden_size=64):\n",
    "        super(A2CNet, self).__init__()\n",
    "\n",
    "        # Shared Feature Extractor\n",
    "        self.shared_layer = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Actor Head (Policy: pi(a|s)) - Outputs logits for Categorical distribution\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "\n",
    "        # Critic Head (Value function: V(s)) - Outputs a single state value estimate\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1) # Single output for the value V(s)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "      \"\"\"Passes the state through the network.\"\"\"\n",
    "      shared_features = self.shared_layer(x)\n",
    "      action_logits = self.actor(shared_features)\n",
    "      state_value = self.critic(shared_features)\n",
    "      return action_logits, state_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FJ1yqpbmFNzy"
   },
   "outputs": [],
   "source": [
    "# --- 2. A2C AGENT IMPLEMENTATION ---\n",
    "class A2CAgent:\n",
    "    \"\"\"Handles interaction with the environment, storage, and the learning update.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, config):\n",
    "        # FIX: Store the configuration dictionary for later access (e.g., in evaluate_agent)\n",
    "        self.config = config \n",
    "        \n",
    "        # Determine the device to use\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Extract hyperparameters\n",
    "        self.gamma = config['gamma']\n",
    "        self.lr = config['learning_rate']\n",
    "        self.c_entropy = config.get('c_entropy', 0.01) # ADDED: Entropy Coefficient (use 0.01 default)\n",
    "        self.epsilon_decay = config.get('epsilon_decay', 1.0) \n",
    "        self.replay_memory_size = config.get('replay_memory_size', 1000) \n",
    "        self.batch_size = config.get('batch_size', 64) \n",
    "\n",
    "        # Initialize Model and Optimizer\n",
    "        self.model = A2CNet(state_dim, action_dim, config).to(self.device) \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        # Lists to store episode trajectory data (A2C is an on-policy algorithm)\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.distributions = [] # ADDED: To store distribution objects for entropy calculation\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Selects an action based on the current policy (Actor).\"\"\"\n",
    "        # Ensure state is a tensor on the correct device\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        action_logits, value_tensor = self.model(state_tensor)\n",
    "\n",
    "        # Create a categorical distribution over action logits\n",
    "        dist = Categorical(logits=action_logits)\n",
    "        action = dist.sample()\n",
    "\n",
    "        # Store data for learning step \n",
    "        self.log_probs.append(dist.log_prob(action))\n",
    "        self.values.append(value_tensor)\n",
    "        self.distributions.append(dist) # ADDED: Store the distribution object\n",
    "        \n",
    "        return action.item()\n",
    "\n",
    "    def update(self, next_state):\n",
    "        \"\"\"Performs the A2C update using the collected trajectory.\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Convert lists to tensors, ensuring they are on the correct device\n",
    "        rewards = torch.FloatTensor(self.rewards).to(self.device)\n",
    "        log_probs = torch.cat(self.log_probs).to(self.device)\n",
    "        values = torch.cat(self.values).squeeze().to(self.device)\n",
    "        \n",
    "        # 1. Calculate Discounted Future Rewards (R_t)\n",
    "        # ... (Returns calculated as before) ...\n",
    "        if next_state is None:\n",
    "            R = 0\n",
    "        else:\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                _, R_tensor = self.model(next_state_tensor)\n",
    "                R = R_tensor.item()\n",
    "\n",
    "        returns = []\n",
    "        for r in rewards.flip(dims=(0,)): \n",
    "            R = r.item() + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "\n",
    "        # 2. Calculate Advantage (A_t)\n",
    "        advantage = returns - values\n",
    "        \n",
    "        # Calculate Entropy (H)\n",
    "        entropy = torch.cat([d.entropy() for d in self.distributions]).mean()\n",
    "        \n",
    "        # 3. Calculate Actor Loss (Policy Loss) - FIXED: Added Entropy Regularization\n",
    "        # Loss = - Policy_Gradient_Term - Entropy_Term\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean() - self.c_entropy * entropy\n",
    "\n",
    "        # 4. Calculate Critic Loss (Value Function Loss)\n",
    "        critic_loss = nn.MSELoss()(values, returns)\n",
    "\n",
    "        # 5. Total Loss and Optimization Step\n",
    "        total_loss = actor_loss + 0.5 * critic_loss \n",
    "\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Clear trajectory buffers\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.distributions = [] # ADDED: Clear distributions buffer\n",
    "\n",
    "        return total_loss.item()\n",
    "\n",
    "    def store_transition(self, reward):\n",
    "        \"\"\"Stores the reward for the current step.\"\"\"\n",
    "        self.rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Y1N8e9N3FVsv"
   },
   "outputs": [],
   "source": [
    "def train_agent(env, agent, num_episodes, env_name, max_steps=500):\n",
    "    \"\"\"The main training loop.\"\"\"\n",
    "    print(f\"\\n--- Starting Training on {env_name} ---\")\n",
    "\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        seed_arg = GLOBAL_SEED if i_episode == 1 else None\n",
    "\n",
    "        # Reset the environment at the start of the episode loop\n",
    "        state, _ = env.reset(seed=seed_arg)\n",
    "\n",
    "        episode_reward = 0\n",
    "        loss = 0\n",
    "\n",
    "        # Loop for a single episode (t: timestep)\n",
    "        for t in itertools.count():\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.store_transition(reward)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                # Update the network when the episode is finished\n",
    "                loss = agent.update(next_state if not terminated else None)\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Log episode results to Wandb\n",
    "        wandb.log({\n",
    "            \"episode\": i_episode,\n",
    "            \"episode_reward\": episode_reward,\n",
    "            # Use the length of the episode (t) for average loss calculation\n",
    "            \"avg_step_loss\": loss / t if t > 0 else 0,\n",
    "            \"episode_length\": t,\n",
    "        })\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f\"Episode: {i_episode}/{num_episodes} | Reward: {episode_reward:.2f}\")\n",
    "\n",
    "    print(f\"--- Training finished for {agent.model.__class__.__name__} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.min_position = -1.2 # The minimum position of the track\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # --- REWARD SHAPING LOGIC ---\n",
    "        \n",
    "        # 1. Base Reward (The default -1 per step is implicit in the environment's max_steps limit)\n",
    "        # We can explicitly set it to a small penalty if we want\n",
    "        shaped_reward = -1.0 \n",
    "        \n",
    "        # 2. Add Reward for progress towards the goal (position is between -1.2 and 0.5)\n",
    "        # Position is s[0]. The goal is at 0.5.\n",
    "        # Max reward is given when the position is at 0.5.\n",
    "        \n",
    "        # This gives a reward that encourages moving right (max position)\n",
    "        shaped_reward += 100 * (observation[0] - self.min_position)\n",
    "        \n",
    "        # 3. Terminal Reward\n",
    "        if terminated:\n",
    "            shaped_reward += 1000 # Large reward for success\n",
    "        \n",
    "        # NOTE: The default environment reward is still in 'reward', so we replace it with 'shaped_reward'\n",
    "        return observation, shaped_reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HKUGGlJDFZiD"
   },
   "outputs": [],
   "source": [
    "# --- UPDATED: evaluate_agent Function (Cell 22) ---\n",
    "def evaluate_agent(env_name, agent, num_tests=100, record_video=False):\n",
    "    \"\"\"Evaluates the trained agent for a number of episodes and logs duration.\"\"\"\n",
    "    print(f\"\\n--- Starting Evaluation for {agent.config.model} on {env_name} ({num_tests} tests) ---\")\n",
    "\n",
    "    # Determine render mode\n",
    "    render_mode = \"rgb_array\" if record_video else None\n",
    "\n",
    "    # 1. Create evaluation environment (must use render_mode='rgb_array' for video)\n",
    "    eval_env = gym.make(env_name, render_mode=render_mode)\n",
    "    \n",
    "    # 2. APPLY WRAPPERS (Matching training environment setup)\n",
    "    if env_name == \"Pendulum-v1\":\n",
    "         class ContinuousActionWrapper(gym.ActionWrapper):\n",
    "             def __init__(self, env):\n",
    "                 super().__init__(env)\n",
    "                 # Ensure this action_range matches your training setup: [-2.0, -1.0, 0.0, 1.0, 2.0]\n",
    "                 self.action_range = [-2.0, -1.0, 0.0, 1.0, 2.0]\n",
    "                 self.action_space = gym.spaces.Discrete(len(self.action_range))\n",
    "             def action(self, action_idx):\n",
    "                 return np.array([self.action_range[action_idx]], dtype=np.float32)\n",
    "\n",
    "         eval_env = ContinuousActionWrapper(eval_env)\n",
    "         \n",
    "    elif env_name == \"MountainCar-v0\":\n",
    "        # === CRITICAL FIX: RE-APPLY THE REWARD SHAPING WRAPPER HERE ===\n",
    "        eval_env = MountainCarRewardWrapper(eval_env) \n",
    "\n",
    "    # Wrap for collecting episode statistics\n",
    "    eval_env = RecordEpisodeStatistics(eval_env)\n",
    "\n",
    "    # Wrap for video recording (if requested)\n",
    "    if record_video:\n",
    "        video_folder = f\"./videos/{env_name}_{agent.config.model}\"\n",
    "        # Only record the first test episode\n",
    "        eval_env = RecordVideo(\n",
    "            eval_env,\n",
    "            video_folder=video_folder,\n",
    "            episode_trigger=lambda x: x == 0,\n",
    "            name_prefix=f\"best_agent\"\n",
    "        )\n",
    "        print(f\"Recording the first episode to: {video_folder}\")\n",
    "\n",
    "    # Run tests\n",
    "    test_durations = []\n",
    "    test_rewards = []\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    agent.model.eval()\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        state, info = eval_env.reset()\n",
    "        done = False\n",
    "        current_episode_reward = 0 # Initialize episode reward for this test\n",
    "\n",
    "        while not done:\n",
    "            # Move state to the agent's device\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "\n",
    "            with torch.no_grad(): # No need to track gradients during evaluation\n",
    "                action_logits, _ = agent.model(state_tensor)\n",
    "\n",
    "            # Select deterministic action by taking the argmax\n",
    "            action = torch.argmax(action_logits).item()\n",
    "\n",
    "            # Note: The reward returned here is the WRAPPED reward if the wrapper is applied above\n",
    "            state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            current_episode_reward += reward\n",
    "\n",
    "            # RecordEpisodeStatistics wrapper adds episode stats to info on done=True\n",
    "            if done:\n",
    "                # The stats are typically stored in the info dictionary\n",
    "                if 'episode' in info:\n",
    "                    duration = info['episode']['l']\n",
    "                    reward = info['episode']['r']\n",
    "\n",
    "                    # Ensure duration and reward are floats\n",
    "                    duration = float(duration)\n",
    "                    reward = float(reward)\n",
    "\n",
    "                    test_durations.append(duration)\n",
    "                    test_rewards.append(reward)\n",
    "\n",
    "                    # Log individual test result\n",
    "                    wandb.log({\n",
    "                        f\"{env_name}/Test_Episode_Duration\": duration,\n",
    "                        f\"{env_name}/Test_Episode_Reward\": reward,\n",
    "                        \"test_episode_index\": i\n",
    "                    })\n",
    "                break\n",
    "\n",
    "    eval_env.close()\n",
    "\n",
    "    # Put model back in training mode\n",
    "    agent.model.train()\n",
    "\n",
    "    if test_durations:\n",
    "        avg_duration = np.mean(test_durations)\n",
    "        std_duration = np.std(test_durations)\n",
    "        avg_reward = np.mean(test_rewards)\n",
    "\n",
    "        wandb.log({\n",
    "            f\"{env_name}/Avg_Test_Duration\": avg_duration,\n",
    "            f\"{env_name}/Std_Test_Duration\": std_duration,\n",
    "            f\"{env_name}/Avg_Test_Reward\": avg_reward,\n",
    "        })\n",
    "\n",
    "        print(f\"Evaluation complete. Avg Duration: {avg_duration:.2f} \\u00b1 {std_duration:.2f} steps.\")\n",
    "        print(f\"Avg Reward: {avg_reward:.2f}\")\n",
    "\n",
    "    return avg_reward, avg_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-Qq5kElZFdzx"
   },
   "outputs": [],
   "source": [
    "def main_run(config):\n",
    "    \"\"\"Initializes Wandb, environment, agent, trains, and evaluates.\"\"\"\n",
    "    # 1. Initialize Wandb Run\n",
    "    run_name = config.get('name', f\"{config['model']}_DF{config['gamma']}_NNLR{config['learning_rate']}\")\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"Cartpole-v1-problem-seed-right-value-v3\",\n",
    "        name=run_name,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    # 2. Setup Environment\n",
    "    env_name = config['env_name']\n",
    "\n",
    "    if env_name in [\"Pendulum-v1\", \"MountainCar-v0\"]:\n",
    "        # Discretize continuous environments for Q-Learning\n",
    "        if env_name == \"Pendulum-v1\":\n",
    "             # Actions: 5 discrete actions: max_torque * [-2.0, -1.0, 0.0, 1.0, 2.0]\n",
    "             env = gym.make(env_name, max_episode_steps=200) # Default\n",
    "             class ContinuousActionWrapper(gym.ActionWrapper):\n",
    "                 def __init__(self, env):\n",
    "                     super().__init__(env)\n",
    "                     self.action_range = [-2.0, -1.0, 0.0, 1.0, 2.0]\n",
    "                     self.action_space = gym.spaces.Discrete(len(self.action_range))\n",
    "                 def action(self, action_idx):\n",
    "                     # Map the discrete index to the continuous action value\n",
    "                     return np.array([self.action_range[action_idx]], dtype=np.float32)\n",
    "             env = ContinuousActionWrapper(env)\n",
    "\n",
    "        elif env_name == \"MountainCar-v0\":\n",
    "            # Actions: 3 discrete actions: 0:push_left, 1:no_push, 2:push_right\n",
    "            env = gym.make(env_name, max_episode_steps=200) # Default\n",
    "            env = MountainCarRewardWrapper(env) \n",
    "            \n",
    "    elif env_name in [\"CartPole-v1\", \"Acrobot-v1\"]:\n",
    "        env = gym.make(env_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Environment {env_name} not supported by this script.\")\n",
    "\n",
    "    # Get environment specs\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # 3. Create Agent\n",
    "    agent = A2CAgent(state_size, action_size, wandb.config)\n",
    "\n",
    "    # 4. Train\n",
    "    #Set episodes based on difficulty (you can adjust these)\n",
    "    if env_name == \"CartPole-v1\":\n",
    "        num_episodes = 500\n",
    "    elif env_name == \"Acrobot-v1\":\n",
    "        num_episodes = 1000\n",
    "    elif env_name == \"MountainCar-v0\":\n",
    "        num_episodes = 3000\n",
    "    elif env_name == \"Pendulum-v1\":\n",
    "        num_episodes = 1000\n",
    "\n",
    "    train_agent(env, agent, num_episodes, env_name)\n",
    "    env.close()\n",
    "\n",
    "    # 5. Evaluate (100 tests)\n",
    "    evaluate_agent(env_name, agent, num_tests=100, record_video=True)\n",
    "\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GHTCtIKQFlQK"
   },
   "outputs": [],
   "source": [
    "BASELINE_CONFIG = {\n",
    "    \"env_name\": \"CartPole-v1\",       # Environment to run (change this for other envs)\n",
    "    \"model\": \"A2C\",                 \n",
    "    \"num_episodes\": 500,            # Training episodes (overwritten in main_run)\n",
    "    \"gamma\": 0.999,                  # CHANGED: Increased from 0.99 for long horizon\n",
    "    \"learning_rate\": 5e-4,           # CHANGED: Increased from 2e-4 for faster convergence\n",
    "    \"c_entropy\": 0.01,               # ADDED: Entropy coefficient for exploration\n",
    "    \"epsilon_start\": 1.0,           # (Ignored by A2C)\n",
    "    \"epsilon_end\": 0.01,            # (Ignored by A2C)\n",
    "    \"epsilon_decay\": 0.001,         # (Ignored by A2C)\n",
    "    \"memory_size\": 50000,           # (Ignored by A2C)\n",
    "    \"batch_size\": 64,               # (Ignored by A2C)\n",
    "    \"target_update_freq\": 200,      # (Ignored by A2C)\n",
    "    \"seed\": 100,\n",
    "}\n",
    "\n",
    "MOUNTAINCAR_BASELINE_CONFIG = {\n",
    "        \"env_name\": \"MountainCar-v0\",\n",
    "        \"model\": \"A2C\",\n",
    "        \"num_episodes\": 5000,\n",
    "        \"gamma\": 1.0,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"c_entropy\": 0.2,\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.01,\n",
    "        \"epsilon_decay\": 0.0005,\n",
    "        \"memory_size\": 50000,\n",
    "        \"batch_size\": 32,\n",
    "        \"target_update_freq\": 200,\n",
    "        \"seed\": 100,\n",
    "    }\n",
    "\n",
    "ACROBOT_BASELINE_CONFIG = {\n",
    "        \"env_name\": \"Acrobot-v1\",\n",
    "        \"model\": \"A2C\",\n",
    "        \"num_episodes\": 1000,\n",
    "        \"gamma\": 0.999,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"c_entropy\": 0.05,\n",
    "        \"seed\": 100,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "xPpc0XrQFt2r",
    "outputId": "4fe122fd-5b48-4bd9-8980-b6c60b81c8c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================\n",
      "Starting Experiment 1/1: AB_R1_A2C_BASELINE_OPTM\n",
      "========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamira-elgarf02\u001b[0m (\u001b[33mamira-elgarf02-cairo-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "d:\\anaconda_\\Lib\\site-packages\\pydantic\\main.py:308: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\uni_courses\\sem_9\\reinforcement learning\\assignments\\assignment_3\\wandb\\run-20251204_042748-hbv3x5ep</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v3/runs/hbv3x5ep' target=\"_blank\">AB_R1_A2C_BASELINE_OPTM</a></strong> to <a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v3' target=\"_blank\">https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v3/runs/hbv3x5ep' target=\"_blank\">https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v3/runs/hbv3x5ep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training on Acrobot-v1 ---\n",
      "Episode: 100/1000 | Reward: -500.00\n",
      "Episode: 200/1000 | Reward: -218.00\n",
      "Episode: 300/1000 | Reward: -384.00\n",
      "Episode: 400/1000 | Reward: -119.00\n",
      "Episode: 500/1000 | Reward: -109.00\n",
      "Episode: 600/1000 | Reward: -110.00\n",
      "Episode: 700/1000 | Reward: -115.00\n",
      "Episode: 800/1000 | Reward: -89.00\n",
      "Episode: 900/1000 | Reward: -90.00\n",
      "Episode: 1000/1000 | Reward: -73.00\n",
      "--- Training finished for A2CNet ---\n",
      "\n",
      "--- Starting Evaluation for A2C on Acrobot-v1 (100 tests) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda_\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at d:\\uni_courses\\sem_9\\reinforcement learning\\assignments\\assignment_3\\videos\\Acrobot-v1_A2C folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording the first episode to: ./videos/Acrobot-v1_A2C\n",
      "Evaluation complete. Avg Duration: 84.58 ± 16.43 steps.\n",
      "Avg Reward: -83.58\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Acrobot-v1/Avg_Test_Duration</td><td>▁</td></tr><tr><td>Acrobot-v1/Avg_Test_Reward</td><td>▁</td></tr><tr><td>Acrobot-v1/Std_Test_Duration</td><td>▁</td></tr><tr><td>Acrobot-v1/Test_Episode_Duration</td><td>▁▂▂▂▂▂▂▂▅▂█▂▂▃▂▂▃▃▃▂▂▁▂▂▂▁▂▂▂▂▂▂▁▂▂▂▂▂▂▁</td></tr><tr><td>Acrobot-v1/Test_Episode_Reward</td><td>▆▆▇▆▆▇▇▇▆▇▆▅▇▇▆▆▅▇▇▅█▄▄▇▆▇▆▇▄▆▇▆▆▆▇▆▆▁▇▆</td></tr><tr><td>avg_step_loss</td><td>▅███▇█▆▆▄▄▃▂▂▄▂▃▂▁▁▂▁▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>episode_length</td><td>██▅███████▃▆▂▂▂▂▂▂▂▂▂▁▂▂▃▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁</td></tr><tr><td>episode_reward</td><td>▁▁▂▁▁▁▁▁▅▆▆▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▆█▇▇██</td></tr><tr><td>test_episode_index</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Acrobot-v1/Avg_Test_Duration</td><td>84.58</td></tr><tr><td>Acrobot-v1/Avg_Test_Reward</td><td>-83.58</td></tr><tr><td>Acrobot-v1/Std_Test_Duration</td><td>16.43179</td></tr><tr><td>Acrobot-v1/Test_Episode_Duration</td><td>71</td></tr><tr><td>Acrobot-v1/Test_Episode_Reward</td><td>-70</td></tr><tr><td>avg_step_loss</td><td>1.46033</td></tr><tr><td>episode</td><td>1000</td></tr><tr><td>episode_length</td><td>73</td></tr><tr><td>episode_reward</td><td>-73</td></tr><tr><td>test_episode_index</td><td>99</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AB_R1_A2C_BASELINE_OPTM</strong> at: <a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v3/runs/hbv3x5ep' target=\"_blank\">https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v3/runs/hbv3x5ep</a><br> View project at: <a href='https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v3' target=\"_blank\">https://wandb.ai/amira-elgarf02-cairo-university/CartPole-v1-problem-seed-right-value-v3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251204_042748-hbv3x5ep\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ALL 10 CARTPOLE EXPERIMENTS COMPLETE. CHECK WANDB FOR RESULTS.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Ensure videos folder exists\n",
    "    os.makedirs(\"./videos\", exist_ok=True)\n",
    "    # R1: DQN Baseline (Control Group - Optimized for Sparse Reward)\n",
    "    config_r1 = deepcopy(ACROBOT_BASELINE_CONFIG)\n",
    "    config_r1['name'] = 'AB_R1_A2C_BASELINE_OPTM'\n",
    "\n",
    "    # R2: DDQN Baseline (Model Comparison)\n",
    "    config_r2 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    config_r2['model'] = 'DDQN'\n",
    "    config_r2['name'] = 'MC_R2_DDQN_COMPARISON'\n",
    "\n",
    "    # --- DISCOUNT FACTOR (GAMMA) ---\n",
    "    # R3: Gamma Too Low (Myopic Agent - Expected to Fail)\n",
    "    config_r3 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    config_r3['gamma'] = 0.95  # Significantly lower than 0.999\n",
    "    config_r3['name'] = 'MC_R3_GAMMA_LOW_0.95'\n",
    "\n",
    "    # R4: Gamma Very High (Testing Edge Case)\n",
    "    config_r4 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    config_r4['gamma'] = 1.0 # Perfect Discount\n",
    "    config_r4['name'] = 'MC_R4_GAMMA_PERFECT_1.0'\n",
    "\n",
    "    # --- NN LEARNING RATE (LR) ---\n",
    "    # R5: LR Too High (Divergence/Oscillation)\n",
    "    config_r5 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    config_r5['learning_rate'] = 0.01\n",
    "    config_r5['name'] = 'MC_R5_LR_HIGH_0.01'\n",
    "\n",
    "    # R6: LR Too Low (Slow Convergence)\n",
    "    config_r6 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    config_r6['learning_rate'] = 1e-5\n",
    "    config_r6['name'] = 'MC_R6_LR_LOW_1e-5'\n",
    "\n",
    "    # --- EPSILON DECAY RATE (ALPHA) ---\n",
    "    # R7: Decay Too Slow (Persistent Exploration - Baseline is already slow, make it ultra-slow)\n",
    "    config_r7 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    config_r7['epsilon_decay'] = 0.0001\n",
    "    config_r7['name'] = 'MC_R7_DECAY_ULTRA_SLOW_0.0001'\n",
    "\n",
    "    # R8: Decay Too Fast (Premature Exploitation - Expected to Fail)\n",
    "    config_r8 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    config_r8['epsilon_decay'] = 0.01 # 10x faster than baseline\n",
    "    config_r8['name'] = 'MC_R8_DECAY_FAST_0.01'\n",
    "\n",
    "    # --- MEMORY SIZE & BATCH SIZE ---\n",
    "    # R9: Small Replay Memory (High Correlation)\n",
    "    config_r9 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    config_r9['memory_size'] = 5000\n",
    "    config_r9['name'] = 'MC_R9_MEM_SMALL_5k'\n",
    "\n",
    "    # R10: Large Batch Size (Smoother Gradients, but may hinder exploration on sparse rewards)\n",
    "    config_r10 = deepcopy(MOUNTAINCAR_BASELINE_CONFIG)\n",
    "    config_r10['batch_size'] = 128\n",
    "    config_r10['name'] = 'MC_R10_BATCH_LARGE_128'\n",
    "\n",
    "\n",
    "    # --- EXECUTION LOOP ---\n",
    "    #experiment_configs = [\n",
    "     #   config_r1, config_r2, config_r3, config_r4, config_r5,\n",
    "      #  config_r6, config_r7, config_r8, config_r9, config_r10\n",
    "    #]\n",
    "\n",
    "    experiment_configs = [\n",
    "        config_r1\n",
    "    ]\n",
    "\n",
    "    for i, config in enumerate(experiment_configs):\n",
    "        print(f\"\\n========================================================\")\n",
    "        print(f\"Starting Experiment {i+1}/{len(experiment_configs)}: {config['name']}\")\n",
    "        print(f\"========================================================\")\n",
    "\n",
    "        # Log the specific config name to Wandb for easy identification\n",
    "        config_to_run = deepcopy(config)\n",
    "\n",
    "        # NOTE: If you are running this in a notebook, you may need to restart\n",
    "        # the kernel between runs to ensure Wandb is initialized correctly.\n",
    "\n",
    "        # Run the experiment\n",
    "        main_run(config_to_run)\n",
    "\n",
    "    print(\"\\n\\nALL 10 CARTPOLE EXPERIMENTS COMPLETE. CHECK WANDB FOR RESULTS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14qAhvp0cq2b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
